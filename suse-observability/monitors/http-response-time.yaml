nodes:
- _type: Monitor
  arguments:
    criticalThreshold: 5.0
    criticalTimeWindow: 120 seconds
    deviatingThreshold: 3.0
    deviatingTimeWindow: 60 seconds
    loggingLevel: WARN
    nameTemplate: HTTP - response time - is above \{{threshold\}}
    quantile: 0.95
  description: |-
    It is important to keep an eye on the HTTP response times for a Kubernetes service. SUSE Observability monitors the 95th percentile response time, or Q95, which is a statistical measure indicating that 95% of the responses are faster than this time.
    This is a useful measure for identifying outliers and slow requests that may negatively impact the user experience. If the Q95 response time is above threshold seconds during a specified time window, the monitor will produce a notification indicating a deviating state.
    To understand the full monitor definition check the details.
  function: {{ get "urn:stackpack:prime-kubernetes:shared:monitor-function:http-response-time"  }}
  id: -16
  identifier: urn:custom:monitor:http-response-time-v2
  intervalSeconds: 30
  name: HTTP - response time - is above threshold seconds
  remediationHint: |-
    We have detected that this service has a \{{ quantile \}}th percentile response time above \{{ threshold \}}.
    This signals that a significant number of users are experiencing a slow service.

    Take the following steps to diagnose the problem:

    ## Possible causes

    - Slow dependency or dependency serving errors
    - Recent update of the application
    - Load on the application has increased
    - Code has memory leaks
    - Environment issues (e.g. certain nodes, database or services that the service depends on)

    ### Slow dependency or dependency serving errors

    Check, in the related health violations of this monitor (which can be found in the expanded version if you read this in the pinned minimised version) if there are any health violations on one of the services or pods that this service depends on (focus on the lowest dependency). If you find a violation (deviating or critical health), click on that component to see the related health violations in table next to it. You can than click on those health violations to follow the instructions to resolve the issue.

    ### Recent update of the service

    Check if the service was recently updated:

    - See the Age in the "About" section to identify on the [service highlight page](/#/components/\{{ componentUrnForUrl \}})
      is this is recently deployed
    - Check if any of the pods are recently updated by clicking on "Pods of this service" in "Related resource" section of
      the [service highlight page](/#/components/\{{ componentUrnForUrl \}}) and look if their Age is recent.
    - If application has just started, it might be that the service has not warmed up yet. Compare the response time metrics
      for the current deployment with the previous deployment by checking the response time metric chart with a time interval including both.
    - Check if application is using more resources than before, consider scaling it up or giving it more resources.
    - If increased latency is crucial, consider rolling back the service to the previous version:
      - if that helps, then the issue is likely with new deployment
      - if that does not help, then the issue may be in the environment (e.g. network issues or issues with the underlying infrastructure, e. g. database)

    ### Load on the service has increased

    - Check if the amount of requests to the service has increased by looking at the "Throughput (HTTP responses/s)" chart for the "HTTP response metrics for all clients (incoming requests)" on the [service highlight page](/#/components/\{{ componentUrnForUrl \}}).
      If so, consider scaling up the service or giving it more resources.

    ### Code has memory leaks

    - Check if memory or CPU usage have been increasing over time. If so, there might be a memory leak.
      You can find the pods supporting this service by clicking on "Pods of this service" in "Related resource"
      section of the [service highlight page](/#/components/\{{ componentUrnForUrl \}}).

      Check which pods are using the most disk space by clicking on the left side of the [service highlight page](/#/components/\{{ componentUrnForUrl \}}) on "Pods of this service"
        - Check all the pods supporting this service by clicking on the pod name
        - Check the resource usage on the "Resource usage" section
    - Restart the pod(s) of this service that is having the issue or add more memory/cpu

    ### Environment issues

    - Check latency of particular pods of the service. If only certain pods are having issues, might be an issue with the node the pod is running on:
      - Try to move the pod to another node
      - Check if other pods of other services are also having latency increased on that node. Drain the node if that is the case.
  status: ENABLED
  tags:
  - services
