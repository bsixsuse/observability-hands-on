nodes:
- _type: Monitor
  arguments:
    comparator: GT
    failureState: DEVIATING
    queries:
      aliasTemplate: Error ratio (errors/total)
      promqlQuery: sum(rate(otel_span_calls_total{status_code="STATUS_CODE_ERROR", span_kind=~"SPAN_KIND_SERVER|SPAN_KIND_CONSUMER", k8s_cluster_name="${tags.cluster-name}", k8s_namespace_name="${tags.namespace}", __multi__="${properties.local_pod_metric_selector__local_pod_label_+}"}[2m])  or (present_over_time(otel_span_calls_total{span_kind=~"SPAN_KIND_SERVER|SPAN_KIND_CONSUMER", k8s_cluster_name="${tags.cluster-name}", k8s_namespace_name="${tags.namespace}", __multi__="${properties.local_pod_metric_selector__local_pod_label_+}"}[2m]) -1)) / sum(rate(otel_span_calls_total{span_kind=~"SPAN_KIND_SERVER|SPAN_KIND_CONSUMER", k8s_cluster_name="${tags.cluster-name}", k8s_namespace_name="${tags.namespace}", __multi__="${properties.local_pod_metric_selector__local_pod_label_+}"}[2m])) * 100
      topologyQuery: (label = "stackpack:kubernetes" and type = "service")
      unit: percent
    threshold: 5.0
    titleTemplate: Span error ratio
  description: |
    Percentage of server and consumer spans that are in an error state for a Kubernetes service.
  function: {{ get "urn:stackpack:common:monitor-function:topological-threshold"  }}
  id: -13
  identifier: urn:custom:monitor:k8s-service-span-error-ratio-v2
  intervalSeconds: 60
  name: Span error ratio for Kubernetes services V2
  remediationHint: |
    We have detected that more than \{{threshold\}}% spans for your Kubernetes service have an Error
    status, this may lead to noticeable disruptions or degraded performance for end-users.

    Take the following steps to diagnose the problem:

    ## Possible causes

    - Slow dependency or dependency serving errors
    - Recent update of the application
    - Load on the application has increased
    - Code has memory leaks
    - Environment issues (e.g. certain nodes, database or services that the service depends on)

    ### Slow dependency or dependency serving errors
    When troubleshooting slow dependencies or dependencies that are serving errors, their are two ways to find the root
    cause:

    1. Analyze traces: begin by examining the traces with an error status in the [traces perspective](/#/components/\{{
    componentUrnForUrl\}}/traces?filterBy=status--Error__parentType--External%2CRoot). Expanding a span will show
    the full trace for that span. Spans with an error status are marked and slow services will be displayed as long spans in
    the trace. If you find an erroneous or slow span you can follow the link to the component to continue the investigation.

    2. Review Related Health Violations: Check, in the related health violations of this monitor if there are any health
    violations on one of the services or pods that this service depends on (focus on the lowest dependency). If you find a
    violation to click on the component to see the related health violations. You can than continue your troubleshooting by
    following those health violations. The related health violations can be found in the expanded version if you read this
    in the pinned minimised version.

    ### Recent update of the service

    Check if the service was recently updated:

    - Go to the "Events" section in the middle of the [service highlight page](/#/components/\{{ componentUrnForUrl \}})
    - Check if there was a recent deployment by clicking on the Deployment section.
    - Check if any of the pods are recently updated by clicking on "Pods of this service" in "Related resource" section of
    the [service highlight page](/#/components/\{{ componentUrnForUrl \}}) and look if their Age is recent.
    - If application has just started, it might be that the service has not warmed up yet. Compare the response time metrics
    for the current deployment with the previous deployment by checking the response time metric chart with a time interval
    including both.
    - Check if application is using more resources than before, consider scaling it up or giving it more resources.
    - If increased latency is crucial, consider rolling back the service to the previous version:
    - if that helps, then the issue is likely with new deployment
    - if that does not help, then the issue may be in the environment (e.g. network issues or issues with the underlying
    infrastructure, e. g. database)

    Check if there where recent deployment of any of its related resources:

    - Go to the "Events" section in the middle of the [service highlight page](/#/components/\{{ componentUrnForUrl \}})
    - If the 'Include related resources' option is enabled all events of resources related to this resource like a deployment will also show up and can give you a clue if any change on them is causing this issue.
    - Check if there was a deployment triggering the errors by clicking on the Deployment section. You can see this by checking if there is a correlation between the time of a deployment and a change of behaviour seen by the metrics and events of this service. For easy correlation you can use 'shift'-'click' to add markers to the different graph, log and event widgets.
    - You can see the details of the event (click on the event) to give more information about the issue.


    ### Load on the service has increased

    - Check if the amount of requests to the service has increased by looking at the "Throughput (HTTP responses/s)" chart
    for the "HTTP response metrics for all clients (incoming requests)" on the [service highlight page](/#/components/\{{
    componentUrnForUrl \}}).
    If so, consider scaling up the service or giving it more resources.

    ### Code has memory leaks

    - Check if memory or CPU usage have been increasing over time. If so, there might be a memory leak.
    You can find the pods supporting this service by clicking on "Pods of this service" in "Related resource"
    section of the [service highlight page](/#/components/\{{ componentUrnForUrl \}}).

    Check which pods are using the most disk space by clicking on the left side of the [service highlight
    page](/#/components/\{{ componentUrnForUrl \}}) on "Pods of this service"
    - Check all the pods supporting this service by clicking on the pod name
    - Check the resource usage on the "Resource usage" section
    - Restart the pod(s) of this service that is having the issue or add more memory/cpu

    ### Environment issues

    - Check latency of particular pods of the service. If only certain pods are having issues, might be an issue with the
    node the pod is running on:
    - Try to move the pod to another node
    - Check if other pods of other services are also having latency increased on that node. Drain the node if that is the
    case.
  status: ENABLED
  tags:
  - traces
  - service
