nodes:
- _type: Monitor
  arguments:
    comparator: GTE
    failureState: DEVIATING
    metric:
      aliasTemplate: OOM Killed count
      query: max(increase(kubernetes_containers_last_state_terminated{reason="OOMKilled"}[10m]))
        by (cluster_name, namespace, pod_name, container)
      unit: short
    threshold: 1.0
    urnTemplate: urn:kubernetes:/${cluster_name}:${namespace}:pod/${pod_name}
  description: |-
    It is important to ensure that the containers running in your Kubernetes cluster have enough memory to function properly. Out of memory (OOM) conditions can cause containers to crash or become unresponsive, leading to restarts and potential data loss.
    To monitor for these conditions, we set up a check that detects and reports OOM events in the containers running in the cluster. This check will help you identify any containers that are running out of memory and allow you to take action to prevent issues before they occur.
    To understand the full monitor definition check the details.
  function: {{ get "urn:stackpack:common:monitor-function:threshold"  }}
  id: -13
  identifier: urn:custom:monitor:out-of-memory-for-containers-v2
  intervalSeconds: 30
  name: Out of memory for containers V2
  remediationHint: |-
    An Out of Memory (OOM) event in Kubernetes occurs when a container's memory usage exceeds the limit set for it.
    The Linux kernel's OOM killer process is triggered, which attempts to free up memory by killing one or more processes.
    This can cause the container to terminate, leading to issues such as lost data, service interruption, and increased
    resource usage.

    Common issues that can cause this problem include:
    1. New deployments that introduce a memory leak.
    2. Elevated traffic that causes a temporary increase of memory usage.
    3. Incorrectly configured memory limits.

    ## 1. New deployments that introduce a memory leak
    A memory leak can be recognized by looking at the "Memory Usage" metric shown above. If the metric resembles a saw-tooth pattern that is a clear indication of a memory leak being present in your application. The memory usage increases over time, but the memory is not released until the container is restarted. You will notice that the container continually restarts. If this behaviour is new, it is likely that a new deployment introduced a memory leak.

    This can be checked by looking at the Events shown on the [pod highlights page](/#/components/\{{ componentUrnForUrl \}}/highlights) and checking whether a `Deployment` event happened recently after which the memory usage behaviour changed.

    If the memory leak is caused by a deployment, you can investigate which change led to the memory leak by clicking the "Show last change" button, which will highlight the latest changeset for the deployment. You can then revert the change or fix the memory leak.

    ## 2. Elevated traffic that causes a temporary increase of memory usage
    This can be checked by looking at the "Network Throughput for pods (received)" metric on the [pod metrics page](/#/components/\{{ componentUrnForUrl \}}/metrics) and comparing the usage to the "Memory Usage" metric. If the memory usage increases at the same time as the network throughput, it is likely that the memory usage is caused by the increased traffic.

    As a temporary fix you can elevate the memory limit for the container. However, this is not a long-term solution as the memory usage will likely increase again in the future. You can also consider using Kubernetes autoscaling feature to scale up and down the number of replicas based on resource usage.

    ## 3. Incorrectly configured memory Limits
    This can be checked by looking at the "Memory Usage" metric on the [pod metrics page](/#/components/\{{ componentUrnForUrl \}}/metrics) and comparing the usage to the requests and limits set for the pod. If the memory usage is higher than the limit set for the pod, the container will be terminated by the OOM killer.

    To fix this issue, you can increate the memory limit for the pod, by changing the Kubernetes resource YAML and increasing the memory limit values e.g.
    ```
      metadata:
      …
      spec:
        containers:
            …
            resources:
              limits:
                cpu: "2"
                memory: "3Gi"
              requests:
                cpu: "2"
                memory: "3Gi"
    ```
  status: ENABLED
  tags:
  - containers
  - pods
