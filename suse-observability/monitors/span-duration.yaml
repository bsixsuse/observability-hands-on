nodes:
- _type: Monitor
  arguments:
    comparator: GT
    failureState: DEVIATING
    queries:
      aliasTemplate: Span duration 95th percentile
      promqlQuery: histogram_quantile(0.95, sum(rate(otel_span_duration_milliseconds_bucket{span_kind=~"SPAN_KIND_SERVER|SPAN_KIND_CONSUMER",
        k8s_cluster_name="${tags.cluster-name}", k8s_namespace_name="${tags.namespace}",
        __multi__="${properties.local_pod_metric_selector__local_pod_label_+}"}[1m]))
        by (span_name, le))
      topologyQuery: (label = "stackpack:kubernetes" and type = "service")
      unit: ms
    threshold: 2000.0
    titleTemplate: "Span duration for '${span_name}'"
  description: |
    Monitors the duration of the server and consumer spans. When the 95th percentile of the duration is greater than the threshold (default 5000ms) the monitor has a Deviating state.
  function: {{ get "urn:stackpack:common:monitor-function:topological-threshold"  }}
  id: -13
  identifier: urn:custom:monitor:k8s-service-span-duration-v2
  intervalSeconds: 60
  name: Span duration for Kubernetes services v2
  remediationHint: |-
    We have detected that this service has a 95th percentile span duration above \{{threshold\}}ms for spans.
    This signals that a significant number of users are experiencing a slow service.

    Take the following steps to diagnose the problem:

    ## Possible causes

    - Slow dependency or dependency serving errors
    - Recent update of the application
    - Load on the application has increased
    - Code has memory leaks
    - Environment issues (e.g. certain nodes, database or services that the service depends on)

    ### Slow dependency or dependency serving errors

    When troubleshooting slow dependencies or dependencies that are serving errors, their are two ways to find the root
    cause:

    1. Analyze traces: begin by examining the traces with an error status in the [traces
    perspective](/#/components/\{{componentUrnForUrl\}}/traces?filterBy=duration--\{{ multiply threshold 1000000
    \}}__parentType--External%2CRoot). Expanding a span will show
    the full trace for that span. Spans with an error status are marked and slow services will be displayed as long spans in
    the trace. If you find an erroneous or slow span you can follow the link to the component to continue the investigation.

    2. Review Related Health Violations: Check, in the related health violations of this monitor if there are any health
    violations on one of the services or pods that this service depends on (focus on the lowest dependency). If you find a
    violation to click on the component to see the related health violations. You can than continue your troubleshooting by
    following those health violations. The related health violations can be found in the expanded version if you read this
    in the pinned minimised version.

    ### Recent update of the service

    Check if the service was recently updated:

    - See the Age in the "About" section to identify on the [service highlight page](/#/components/\{{ componentUrnForUrl \}})
    is this is recently deployed
    - Check if any of the pods are recently updated by clicking on "Pods of this service" in "Related resource" section of
    the [service highlight page](/#/components/\{{ componentUrnForUrl \}}) and look if their Age is recent.
    - If application has just started, it might be that the service has not warmed up yet. Compare the response time metrics
    for the current deployment with the previous deployment by checking the response time metric chart with a time interval
    including both.
    - Check if application is using more resources than before, consider scaling it up or giving it more resources.
    - If increased latency is crucial, consider rolling back the service to the previous version:
    - if that helps, then the issue is likely with new deployment
    - if that does not help, then the issue may be in the environment (e.g. network issues or issues with the underlying
    infrastructure, e. g. database)

    ### Load on the service has increased

    - Check if the amount of requests to the service has increased by looking at the "Throughput (HTTP responses/s)" chart
    for the "HTTP response metrics for all clients (incoming requests)" on the [service highlight page](/#/components/\{{
    componentUrnForUrl \}}).
    If so, consider scaling up the service or giving it more resources.

    ### Code has memory leaks

    - Check if memory or CPU usage have been increasing over time. If so, there might be a memory leak.
    You can find the pods supporting this service by clicking on "Pods of this service" in "Related resource"
    section of the [service highlight page](/#/components/\{{ componentUrnForUrl \}}).

    Check which pods are using the most disk space by clicking on the left side of the [service highlight
    page](/#/components/\{{ componentUrnForUrl \}}) on "Pods of this service"
    - Check all the pods supporting this service by clicking on the pod name
    - Check the resource usage on the "Resource usage" section
    - Restart the pod(s) of this service that is having the issue or add more memory/cpu

    ### Environment issues

    - Check latency of particular pods of the service. If only certain pods are having issues, might be an issue with the
    node the pod is running on:
    - Try to move the pod to another node
    - Check if other pods of other services are also having latency increased on that node. Drain the node if that is the
    case.
  status: ENABLED
  tags:
  - traces
  - service
